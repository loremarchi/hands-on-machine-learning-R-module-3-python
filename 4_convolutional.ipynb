{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Convolutional neural networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ANNs, our first step in the MNIST analysis was to flatten the image matrix into a vector. This approach is not translation invariant: \n",
    "* a completely different set of nodes gets activated when the image is shifted.\n",
    "* ignores the dependency between nearby pixels.\n",
    "* requires a large number of parameters/weights aseach node in the first hidden layer is connected to all nodes in the input layer\n",
    "\n",
    "Convolutional layers allow to handle multidimensional data, without flattening!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2d convolutional layers use 2 dimensional input (for example images) to construct 2 dimensional feature maps:\n",
    "* The weights in a 2d convolutional layer are structured in a small image, called the kernel or the filter:\n",
    "<img src=\"./imgs/cnn_1.png\" alt=\"cnn_1\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We slide the kernel over the input image, multiply the selected part of the image and the kernel elementwise and sum:\n",
    "<img src=\"./imgs/cnn_2.png\" alt=\"cnn_2\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "from keras import backend as K\n",
    "\n",
    "seed = 123\n",
    "\n",
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "reset_random_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10000, 28, 28, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand dimension to fourth axis\n",
    "x_train = tf.expand_dims(x_train, axis = -1, name=None); x_train.shape # -1 to add dimension 'on the right side'\n",
    "x_test = tf.expand_dims(x_test, axis = -1, name=None); x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim x_train:(60000, 28, 28, 1), dim x_test:(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'dim x_train:{x_train.shape}, dim x_test:{x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train\n",
    "output_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "output_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fitting a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit our CNN we need to add a 2D convolutional layer:\n",
    "\n",
    "```python\n",
    "model.add(tf.keras.layers.Conv2D(filters = 8, \n",
    "                                 kernel_size = 3,\n",
    "                                 strides = c(1, 1),\n",
    "                                 input_shape = (28, 28, 1)))\n",
    "```\n",
    "* **filters = 8**: We construct 8 feature maps associated to different kernels/filters.\n",
    "* **kernel_size = c(3, 3)**: The filter/kernel has a size of 3x3.\n",
    "* **strides = c(1, 1)**: We move the kernel in steps of 1 pixel in both the horizontal and vertical direction. This is a common choice.\n",
    "* **input_shape = (28, 28, 1)**: If this is the first layer of the model, we also have to specify the dimensions of the input data. The input consists of 1 image of size 28x28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution layer is typically followed by a pooling step, which reduces the size of feature maps. Pooling layers divide the image in blocks of equal size\n",
    "and then aggregate the data per block. Two common operations are:\n",
    "* average pooling\n",
    "* max pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all features are extracted, the data is flattened.This data can be seen as engineered features, automatically created by the CNN architecture. In a next step, a feed-forward ANN is used to analyze these local features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_random_seeds(seed)\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.keras.backend.set_session(session_conf)\n",
    "\n",
    "# fitting convolutional neural netword\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters = 8, \n",
    "                                 kernel_size = 3, \n",
    "                                 input_shape = (28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = 2,\n",
    "                                      strides = 2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'RMSprop',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 5s 104us/sample - loss: 0.4929 - accuracy: 0.8677 - val_loss: 0.2869 - val_accuracy: 0.9162\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 5s 94us/sample - loss: 0.2859 - accuracy: 0.9169 - val_loss: 0.2582 - val_accuracy: 0.9259\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 4s 91us/sample - loss: 0.2528 - accuracy: 0.9274 - val_loss: 0.2263 - val_accuracy: 0.9366\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 4s 94us/sample - loss: 0.2254 - accuracy: 0.9355 - val_loss: 0.2027 - val_accuracy: 0.9448\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 4s 84us/sample - loss: 0.1970 - accuracy: 0.9436 - val_loss: 0.1802 - val_accuracy: 0.9518\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 4s 85us/sample - loss: 0.1720 - accuracy: 0.9508 - val_loss: 0.1629 - val_accuracy: 0.9578\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 4s 88us/sample - loss: 0.1501 - accuracy: 0.9571 - val_loss: 0.1443 - val_accuracy: 0.9620\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 4s 85us/sample - loss: 0.1330 - accuracy: 0.9617 - val_loss: 0.1327 - val_accuracy: 0.9638\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 4s 85us/sample - loss: 0.1204 - accuracy: 0.9659 - val_loss: 0.1244 - val_accuracy: 0.9672\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 4s 86us/sample - loss: 0.1093 - accuracy: 0.9688 - val_loss: 0.1126 - val_accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23ffd4aef48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = x_train,\n",
    "          y = output_train,\n",
    "          epochs = 10,\n",
    "          batch_size = 128,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.11180946557745337, accuracy:0.967199981212616\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance\n",
    "test_loss, test_accuracy = model.evaluate(x_test, output_test, verbose = 0)\n",
    "print(f'loss:{test_loss}, accuracy:{test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reproducibility\n",
    "# 10 epochs - loss:0.11180946557745337, accuracy:0.967199981212616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using test set\n",
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction vs actual\n",
    "predicted_category = np.argmax(prediction, axis = 1)\n",
    "actual_category = np.argmax(output_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mis-predicted values\n",
    "mask = np.equal(predicted_category,actual_category)\n",
    "index_mispred = np.where(~mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of mis-predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reproducibility\n",
    "# 328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulding df predicted/actual categories\n",
    "df = pd.DataFrame({'pred_cat':predicted_category,\n",
    "                   'actual_cat':actual_category})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_cat</th>\n",
       "      <th>actual_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_cat  actual_cat\n",
       "0         7           7\n",
       "1         2           2\n",
       "2         1           1\n",
       "3         0           0\n",
       "4         4           4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_3d_tensor(vector_picture, which, actual_v, predited_v):\n",
    "    pixels = np.array(vector_picture)[which][:][:][:]\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.title(f'actual:{actual_category[which]}, predicted:{predicted_category[which]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_img_3d_tensor(x_test, 2, df['actual_cat'], df['pred_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x236484a0d88>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x236484f1f88>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x236484f1ec8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x23647e0efc8>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters[:, :, :, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d (3, 3, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "#Iterate through all the layers of the model\n",
    "for layer in model.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        weights, bias= layer.get_weights()\n",
    "        print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#normalize filter values between 0 and 1 for visualization\n",
    "f_min, f_max = weights.min(), weights.max()\n",
    "filters = (weights - f_min) / (f_max - f_min)  \n",
    "print(filters.shape[3])\n",
    "filter_cnt=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting all the filters\n",
    "filter_cnt = 1\n",
    "for i in range(filters.shape[3]):\n",
    "    #get the filters\n",
    "    filt=filters[:,:,:,i]\n",
    "    #plotting each of the channel, color image RGB channels\n",
    "    for j in range(filters.shape[0]):\n",
    "        ax= plt.subplot(filters.shape[3], filters.shape[0], filter_cnt)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(filt[:,:,:], cmap = 'Blues')\n",
    "        filter_cnt+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.42008075],\n",
       "        [0.59515166],\n",
       "        [0.3518199 ]],\n",
       "\n",
       "       [[0.29819727],\n",
       "        [0.3070597 ],\n",
       "        [0.02418828]],\n",
       "\n",
       "       [[0.14303386],\n",
       "        [0.45222193],\n",
       "        [0.591597  ]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
